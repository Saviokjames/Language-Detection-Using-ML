# -*- coding: utf-8 -*-
"""colabfull.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TETeMdaukROxtr2Ok-koUZPOa2xAEny5
"""

import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
import matplotlib.pyplot as plt
import seaborn as sns

"""# SPRINT -1
# DATA PREPROCESSING
"""

nltk.download('punkt')

data = pd.read_csv("/content/drive/MyDrive/language  dataset/dataset.csv")

unique_languages = data['language'].unique()

# Print the unique languages
for lang in unique_languages:
    print(lang)

data.head()

"""**LANGUAGE DISTRIBUTION**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Count the number of samples per language
language_counts = data['language'].value_counts()

# Create a bar chart to visualize class distribution
plt.figure(figsize=(12, 6))
sns.barplot(x=language_counts.index, y=language_counts.values)
plt.xlabel('Language')
plt.ylabel('Number of Samples')
plt.title('Language Distribution in the Dataset')
plt.xticks(rotation=90)
plt.show()

"""**LOWERCASING THE DATSET**"""

data['Text'] = data['Text'].str.lower()

"""TOKENIZING THE DATSET"""

data['Text'] = data['Text'].apply(lambda text: word_tokenize(text))

print(data.head())

"""**CREATING STOPWORDS**

"""

# Define stop word lists for each language
stop_words = {

    "swedish": set(nltk.corpus.stopwords.words("swedish")),
    "dutch": set(nltk.corpus.stopwords.words("dutch")),
    "turkish": set(nltk.corpus.stopwords.words("turkish")),
    "indonesian": set(nltk.corpus.stopwords.words("indonesian")),
    "portuguese": set(nltk.corpus.stopwords.words("portuguese")),
    "french": set(nltk.corpus.stopwords.words("french")),
    "chinese": set(nltk.corpus.stopwords.words("chinese")),
    "spanish": set(nltk.corpus.stopwords.words("spanish")),
    "romanian": set(nltk.corpus.stopwords.words("romanian")),
    "russian": set(nltk.corpus.stopwords.words("russian")),
    "english": set(nltk.corpus.stopwords.words("english")),
    "arabic": set(nltk.corpus.stopwords.words("arabic")),
    "estonian": {
        "ja", "ega", "või", "ning", "aga", "sest", "et", "kui",
        "olema", "saama", "oma", "tema", "see", "need", "nende",
    },
    "thai": {
        "และ", "หรือ", "ถ้า", "เพื่อ", "โดย", "แต่", "ซึ่ง",
        "เป็น", "ก็", "ไป", "ที่", "มี", "ใน", "คือ",
    },
    "tamil": {
        "உள்ள", "அவன்", "இந்த", "அவர்", "அது", "அந்த", "இது",
        "ஆனால்", "அந்தரம்", "இனி", "அவனுக்கு", "அவன்",
        "அந்தக்", "அந்தக்",
    },
    "japanese": {
        "と", "に", "は", "を", "が", "で", "の", "て",
        "た", "だ", "り", "ら", "な", "い", "など",
    },
    "latin": {
        "et", "cum", "in", "est", "non", "ad", "ex", "sed",
        "ut", "qui", "hoc", "sunt", "quia", "a", "ab",
    },
    "urdu": {
        "اور", "کی", "سے", "کا", "یا", "کے", "کو", "کی", "کیا", "کہ", "کی", "کے",
    },
    "korean": {
        "그", "그녀", "그것", "이", "저", "우리", "당신", "누구", "어떤", "무엇",
        "왜", "어떻게", "어디", "언제", "모든", "많은", "몇", "적은",
    },
    "hindi": {
        "और", "का", "के", "की", "है", "हैं", "को", "में",
        "से", "कर", "पर", "इस", "की", "एक", "होता", "कि",
    },
    "pushto": {
        "او", "څنګه", "هم", "زموږ", "يو", "خو", "د", "چې",
        "دغه", "دوی", "پر", "به", "لرې", "چه", "دا", "که",
    },
    "persian": {
        "و", "به", "در", "از", "با", "برای", "را", "که", "این", "است",
        "آن", "یک", "هم", "های", "شما", "ما", "می", "آنها", "من", "باشد",
    },
    "nan": set(),  # Placeholder for missing language
}

"""**REMOVING THE STOPWORDS**"""

for lang in stop_words:
    if lang in data.columns:
        data[lang] = data[lang].apply(lambda tokens: [word for word in tokens if word not in stop_words[lang]])

"""**CHECKING MISSING VALUES**"""

missing_values = data.isnull().sum()

# Display the count of missing values for each column
print(missing_values)

missing_values = data.isnull().sum()
percentage_missing = (missing_values / len(data)) * 100
missing_data_summary = pd.DataFrame({
    'Column Name': data.columns,
    'Missing Values': missing_values,
    'Percentage Missing': percentage_missing
})
print("Missing Data Summary:")
print(missing_data_summary)

"""**CHECKING NULL VALUES**"""

null_values = data.isnull().sum()
print(null_values)

"""# FEATURE EXTRACTION USING COUNT VECTORISATION"""

from sklearn.feature_extraction.text import CountVectorizer

x = np.array(data["Text"])
y = np.array(data["language"])
x = [str(text) for text in x]

cv = CountVectorizer()
X = cv.fit_transform(x)

X_array = X.toarray()

X.shape

"""This means that your count vectorized data consists of 22,000 rows (documents) and 278,217 columns (unique words in your vocabulary)"""

word_frequencies = X_array.sum(axis=0)
print(word_frequencies)

"""The first value (4) represents the total count of the first word in the vocabulary.The second value (1) represents the total count of the second word in your vocabulary.

# SPLITING THE DATA INTO TRAINING AND TEST SETS
"""

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.33,
                                                    random_state=42)

model = MultinomialNB()

"""# MODEL TRAINING AND EVALUATION"""

model.fit(X_train, y_train)
y_train_pred = model.predict(X_train)

from sklearn.metrics import accuracy_score
accuracy_train = accuracy_score(y_train, y_train_pred) * 100
print(f"Training Set Accuracy: {accuracy_train:.2f}")

from sklearn.metrics import precision_score
precision_train = precision_score(y_train, y_train_pred, average='weighted') * 100
print(f"Training Set Precision: {precision_train:.4f}")

from sklearn.metrics import f1_score
f1_train = f1_score(y_train, y_train_pred, average='weighted') * 100
print(f"Training Set F1-Score: {f1_train:.2f}%")

from sklearn.metrics import classification_report
report_train = classification_report(y_train, y_train_pred)
print("Training Set Classification Report:")
print(report_train)

from sklearn.metrics import confusion_matrix
class_names = ["Arabic", "Chinese", "Dutch", "English", "Estonian", "French", "Hindi", "Indonesian", "Japanese", "Korean", "Latin", "Persian", "Portuguese", "Pushto", "Romanian", "Russian", "Spanish", "Swedish", "Tamil", "Thai", "Turkish", "Urdu"]
confusion_train = confusion_matrix(y_train, y_train_pred)

plt.figure(figsize=(8, 6))
sns.set(font_scale=1.2)
sns.heatmap(confusion_train, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Training Set Confusion Matrix')
plt.show()

"""#  MODEL TESTING AND EVALUATION"""

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)*100
print(f"Accuracy: {accuracy:.2f}")

from sklearn.metrics import precision_score
precision = precision_score(y_test, y_pred, average='weighted')*100
print(f"Precision: {precision:.4f}")

from sklearn.metrics import f1_score
f1 = f1_score(y_test, y_pred, average='weighted') * 100
print(f"F1-Score: {f1:.2f}%")

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(report)

from sklearn.metrics import confusion_matrix
class_names = ["Arabic", "Chinese", "Dutch", "English", "Estonian", "French", "Hindi", "Indonesian", "Japanese", "Korean", "Latin", "Persian", "Portuguese", "Pushto", "Romanian", "Russian", "Spanish", "Swedish", "Tamil", "Thai", "Turkish", "Urdu"]
confusion = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.set(font_scale=1.2)
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=False,xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()